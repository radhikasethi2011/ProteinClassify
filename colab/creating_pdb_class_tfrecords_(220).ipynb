{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "creating pdb_class tfrecords (220).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOecCoWR4QcNmvidJEykXXh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhikasethi2011/ProteinClassify/blob/main/colab/creating_pdb_class_tfrecords_(220).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GDjhmHr-nUF"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/radhikasethi2011/ProteinClassify.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from statistics import mode\n",
        "import glob as glob"
      ],
      "metadata": {
        "id": "OCwUTcTa_AuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scop_cla = pd.read_csv('/content/ProteinClassify/scop-cla-latest.txt', header=None, skiprows=7, delimiter = ' ')\n",
        "cols = [0,3,4,5,6,7,8,9]\n",
        "scop_cla.drop(scop_cla.columns[cols], axis= 1, inplace=True)\n",
        "scop_cla.rename(columns={ scop_cla.columns[0]: \"residue\" , \n",
        "                         scop_cla.columns[1]: \"chain\",\n",
        "                         scop_cla.columns[2]: \"label\" }, inplace=True)\n",
        "scop_cla['chain'] = scop_cla['chain'].str.split(':').str[0]\n",
        "scop_cla['label'] = scop_cla['label'].str.split(',').str[1].str.split('=').str[1]\n",
        "scop_cla['residue'] = scop_cla['residue'] + '_' + scop_cla['chain']\n",
        "#scop_cla['residue'] = scop_cla['residue'].str[1:]\n",
        "\n",
        "#scop_cla.set_index(['residue'], inplace=True)\n",
        "scop_cla"
      ],
      "metadata": {
        "id": "4wsLvPOY_B2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['domid','pdbid', 'pdbchain']\n",
        "scop_struct = pd.read_csv('/content/ProteinClassify/scop-represented-structures-latest.txt',\n",
        "                   header = None, skiprows=6, names=cols, delimiter = ' ')\n",
        "scop_struct['pdbid'] = scop_struct['pdbid'] + '_' + scop_struct['pdbchain']\n",
        "scop_struct"
      ],
      "metadata": {
        "id": "LPiOaY6f_YOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = scop_cla['residue'] == '5FLV_E'\n",
        "scop_cla[mask]['label']"
      ],
      "metadata": {
        "id": "ky8seHizTBqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = set(list(scop_cla['residue'])).intersection(list(scop_struct['pdbid']))\n",
        "len(ans)"
      ],
      "metadata": {
        "id": "yRe0d7my_ukz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scop_cla.set_index(['residue'], inplace=True)\n",
        "#scop_struct.set_index(['pdbid'], inplace=True)\n",
        "merged = scop_struct.merge(scop_cla, left_on='pdbid', right_on='residue')\n",
        "merged = merged.drop_duplicates(subset=['pdbid'],keep='first')\n",
        "merged.drop(columns=['pdbid','pdbchain'], axis=1, inplace=True)\n",
        "merged['residue'] = merged['residue'].str.split('_').str[0]\n",
        "merged.reset_index(drop=True, inplace=True)\n",
        "merged.to_csv('pdb_chain_class.csv', sep=',')\n",
        "merged.set_index(['residue'], inplace=True)\n",
        "\n",
        "merged"
      ],
      "metadata": {
        "id": "RavLmtZdzUQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = merged.index == '5FLV'\n",
        "merged[mask]['label']"
      ],
      "metadata": {
        "id": "_4rmZnVnOd0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged[mask]['chain']"
      ],
      "metadata": {
        "id": "nDZDDcTTOyky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/pdbs\n",
        "%mkdir /content/ca_csv\n",
        "%mkdir /content/records"
      ],
      "metadata": {
        "id": "1D9eJq796ZMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_atom_record(record):\n",
        "  if record.startswith('ATOM'): \n",
        "    return True;\n",
        "  return False;\n",
        "\n",
        "def is_intended_chain(record, chain):\n",
        "  if record[21] == chain: \n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def is_chain_ter_record(record, chain):\n",
        "  if record[21] == chain and record.startswith(\"TER\"): \n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def is_alt_record(record):\n",
        "  if record[16] == \" \": \n",
        "    return False\n",
        "  return True\n",
        "  \n",
        "def is_ca_atom(record):\n",
        "  if record[12:15].strip() == \"CA\":\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def parse_atom_records(record):\n",
        "  atom = record[12:16].strip()\n",
        "  residue = record[17:20].strip()\n",
        "  chain = record[21].strip()\n",
        "  seq_pos = record[22:26].strip()\n",
        "  x = record[30:38].strip()\n",
        "  y = record[38:46].strip()\n",
        "  z = record[46:54].strip()\n",
        "  return atom, residue, chain, seq_pos, x, y, z\n",
        "\n",
        "def parse_pdb(contents, chainl): #chain: list\n",
        "  ca_records = {}\n",
        "  pos = 0\n",
        "  for i in range(len(chainl)):\n",
        "    for line in contents:\n",
        "      if line.startswith(\"ENDMDL\"): break;\n",
        "      if is_chain_ter_record(line, chainl[i]): break;\n",
        "\n",
        "      if not is_atom_record(line): continue\n",
        "      if not is_intended_chain(line, chainl[i]): continue\n",
        "      if is_alt_record(line): continue \n",
        "      if not is_ca_atom(line): continue\n",
        "\n",
        "      (atom, residue, chain, rel_pos, x, y, z) = parse_atom_records(line)\n",
        "      ca_records[pos] = atom, residue, chain, rel_pos, x, y, z\n",
        "      pos+=1\n",
        "  return ca_records\n",
        "\n",
        "def dump_records_to_csv_file(file_name, record_dict):\n",
        "  file = open(file_name, 'w')\n",
        "  for key in record_dict.keys():\n",
        "    (atom, residue, chain, pos, x, y, z) = record_dict[key]\n",
        "    file.write(f\"%s,%s,%s,%s,%s,%s,%s\\n\"%(atom, residue, chain, pos, x, y, z))\n"
      ],
      "metadata": {
        "id": "NjN5eaJ27bD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "# The following functions can be used to convert a value to a type compatible\n",
        "# with tf.train.Example.\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n"
      ],
      "metadata": {
        "id": "1QHGxKB7-2FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def serialize_example(feature0, feature1, feature3, \n",
        "                      feature4, feature5, feature6):\n",
        "  \"\"\"\n",
        "  Creates a tf.train.Example message ready to be written to a file.\n",
        "  \"\"\"\n",
        "  # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
        "  # data type.\n",
        "  feature = {\n",
        "      'label': _bytes_feature(feature0),\n",
        "      'residue': _bytes_feature(feature1),\n",
        "      'pos': _int64_feature(feature3),\n",
        "      'x': _float_feature(feature4),\n",
        "      'y': _float_feature(feature5),\n",
        "      'z': _float_feature(feature6)\n",
        "  }\n",
        "\n",
        "  # Create a Features message using tf.train.Example.\n",
        "\n",
        "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "  return example_proto.SerializeToString()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NmIvnC_7-6kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filename = 'test.tfrecord'\n",
        "def write_to_testrecord(filename, df, label):\n",
        "  with tf.io.TFRecordWriter(filename) as writer:\n",
        "    for i in range(len(df)):\n",
        "      serialized_example = serialize_example(bytes(label, 'utf-8' ),\n",
        "                                            bytes(df['atom'][i], 'utf-8'), \n",
        "                                            df['pos'][i], df['x'][i], \n",
        "                                            df['y'][i], df['z'][i])\n",
        "      example_proto = tf.train.Example.FromString(serialized_example)\n",
        "      #writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "      writer.write(serialized_example)"
      ],
      "metadata": {
        "id": "ZIAO0Wjp_Dph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edit_pos(csv_name):\n",
        "  col_names = ['ca','atom','residue','pos','x','y','z']\n",
        "  df = pd.read_csv(f'/content/ca_csv/{csv_name}.csv', names=col_names, sep=',')\n",
        "  x = df['x'][0]\n",
        "  y = df['y'][0]\n",
        "  z = df['z'][0]\n",
        "  df['x'] = df['x'] - x\n",
        "  df['y'] = df['y'] - y\n",
        "  df['z'] = df['z'] - z\n",
        "  return df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NzstUl7uH2zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_list = list(merged.index)\n",
        "for i in range(len(pd_list)):\n",
        "  pd_id = pd_list[i]\n",
        "  #print(i, pd_id)\n",
        "  !wget -q https://files.rcsb.org/download/{pd_id}.pdb -O /content/pdbs/{pd_id}.pdb\n",
        "  size = os.path.getsize(f\"/content/pdbs/{pd_id}.pdb\")\n",
        "  if size>0:\n",
        "    chainl = list(merged['chain'][pd_id])\n",
        "    print(\"i: \", i, pd_id, chainl)\n",
        "    with open(f'/content/pdbs/{pd_id}.pdb') as f:\n",
        "      contents = f.readlines() \n",
        "    ca_records = parse_pdb(contents, chainl) \n",
        "    dump_records_to_csv_file(f'/content/ca_csv/{pd_id}.csv', ca_records)\n",
        "    df = edit_pos(pd_id)\n",
        "    label = merged['label'][pd_id]\n",
        "    if(type(label) is str):\n",
        "      filename = '/content/records/' + f'{pd_id}' + '_' + f'{label}' + '.tfrecord'\n",
        "      write_to_testrecord(filename, df, label)\n",
        "    else:\n",
        "      for l in label: \n",
        "        filename = '/content/records/' + f'{pd_id}' + '_' + f'{l}' + '.tfrecord'\n",
        "        write_to_testrecord(filename, df, l)\n",
        "  else: print(\"! ! ! ! file not found at rcsb ! ! ! !\")\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "SNO3I9dM6uol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [filename]\n",
        "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
        "raw_dataset\n",
        "\n",
        "for raw_record in raw_dataset.take(10):\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(raw_record.numpy())\n",
        "  print(example)\n"
      ],
      "metadata": {
        "id": "axP55Ppn6uk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zcvf records.tar.gz /content/records "
      ],
      "metadata": {
        "id": "9Omencj_FMNa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}